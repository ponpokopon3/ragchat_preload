import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

st.title("ğŸ’¬AIãƒãƒ£ãƒƒãƒˆ + RAGï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢å¼·åŒ–ï¼‰")

# --- APIã‚­ãƒ¼å…¥åŠ›æ¬„ ---
api_key = st.text_input(
    "OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå…¥åŠ›å¾Œã€ã‚¨ãƒ³ã‚¿ãƒ¼ã§ã‚»ãƒƒãƒˆï¼‰",
    type="password",
    value=st.session_state.get("api_key", ""),
)
if api_key:
    st.session_state["api_key"] = api_key
if "api_key" not in st.session_state or not st.session_state["api_key"]:
    st.info("OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- äº‹å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ ---
VECTORSTORE_KEY = "vectorstore"
DOC_PATH = "docs/specifications.txt"   # ã“ã“ã‚’å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´

if VECTORSTORE_KEY not in st.session_state:
    with open(DOC_PATH, encoding="utf-8") as f:
        raw_text = f.read()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.create_documents([raw_text])
    embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["api_key"])
    st.session_state[VECTORSTORE_KEY] = FAISS.from_documents(docs, embeddings)
vectorstore = st.session_state[VECTORSTORE_KEY]

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç† ---
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# --- RAGãƒã‚§ãƒ¼ãƒ³ ---
llm = ChatOpenAI(
    openai_api_key=st.session_state["api_key"],
    model="gpt-4o",
    temperature=0.7,
)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å‚è€ƒã«ã€å¿…ãšæ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
    ("human", "ã€å‚è€ƒæƒ…å ±ã€‘\n{context}\n\nã€ä¼šè©±å±¥æ­´ã€‘\n{chat_history}\n\nã€è³ªå•ã€‘\n{question}")
])
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    combine_docs_chain_kwargs={"prompt": prompt}
)

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º ---
for role, content in st.session_state["chat_history"]:
    st.chat_message(role).write(content)

# --- ãƒãƒ£ãƒƒãƒˆå…¥åŠ› ---
user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦")
if user_input:
    st.chat_message("user").write(user_input)
    output = rag_chain.invoke({
        "question": user_input,
        "chat_history": st.session_state["chat_history"]
    })
    answer = output["answer"] if isinstance(output, dict) and "answer" in output else output
    st.chat_message("assistant").write(answer)
    st.session_state["chat_history"].append(("user", user_input))
    st.session_state["chat_history"].append(("assistant", answer))
